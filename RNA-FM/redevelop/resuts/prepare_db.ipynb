{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f3d1a8-86fb-4fdc-ac61-2d8d0a6a488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "def load_pkl(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    # Display basic information\n",
    "    #print(\"Data type:\", type(data))\n",
    "    #print(\"Sample data:\", data[:2] if isinstance(data, list) else data)\n",
    "    return data\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_sequence_embeddings(test_unique_sequences, embeddings_paths):\n",
    "    \"\"\"\n",
    "    Given a list of test unique sequences, filter out sequences longer than 1022 and\n",
    "    create a dictionary mapping each valid sequence to its combined embedding.\n",
    "    \n",
    "    The combined embedding is computed by:\n",
    "      - Max pooling along the sequence length dimension (dim=1)\n",
    "      - Average pooling along the sequence length dimension (dim=1)\n",
    "      - Concatenating the two pooled results along the feature dimension (dim=-1)\n",
    "    \n",
    "    The embedding file for each sequence is expected to be named 'seq_{idx}.npy' in the directory embeddings_paths.\n",
    "    Note: enumerate() starts indexing at 0.\n",
    "    \n",
    "    Parameters:\n",
    "        test_unique_sequences (list): List of sequences.\n",
    "        embeddings_paths (str): Path to the directory containing embedding files.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary where keys are sequences and values are the combined embeddings (torch.Tensor).\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_embeddings = {}\n",
    "    \n",
    "    # Iterate over valid sequences using enumerate (index starts at 0)\n",
    "    for idx, seq in enumerate(test_unique_sequences):\n",
    "        # Filter sequences with length <= 1022\n",
    "        #if len(seq) > 1022:\n",
    "        #    continue\n",
    "        # Construct the filename for the corresponding embedding (with .npy extension)\n",
    "        embedding_file = os.path.join(embeddings_paths, f\"seq_{idx}.npy\")\n",
    "        \n",
    "        # Load the embedding numpy array\n",
    "        embedding_np = np.load(embedding_file)\n",
    "        \n",
    "        # Convert the numpy array to a PyTorch tensor\n",
    "        embedding = torch.from_numpy(embedding_np)\n",
    "        \n",
    "        # Perform max pooling along the sequence length dimension (dim=1)\n",
    "        max_pooled, _ = embedding.max(dim=0)  # Expected shape: (1, emb_dim)\n",
    "        #print('max_pooled', max_pooled.shape)\n",
    "        #print('max_pooled', max_pooled)\n",
    "        \n",
    "        # Perform average pooling along the sequence length dimension (dim=1)\n",
    "        avg_pooled = embedding.mean(dim=0)    # Expected shape: (1, emb_dim)\n",
    "        #print('avg_pooled', avg_pooled.shape)\n",
    "        # print('avg_pooled', avg_pooled)\n",
    "        \n",
    "        # Concatenate the pooled embeddings along the feature dimension (dim=-1)\n",
    "        combined_embedding = torch.cat([max_pooled, avg_pooled], dim=-1)  # Expected shape: (1, emb_dim*2)\n",
    "        #print('combined_embedding', combined_embedding.shape)\n",
    "        # print('combined_embedding', combined_embedding)\n",
    "        \n",
    "        # Add the sequence and its combined embedding to the dictionary\n",
    "        sequence_embeddings[seq] = combined_embedding\n",
    "    \n",
    "    return sequence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33330fd8-f808-4ee8-bf75-6cd806e35968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_full(test_negatives, test_positives, sequence_embeddings_test_dict):\n",
    "    \"\"\"\n",
    "    Create an output list from test_negatives and test_positives pairs. For each pair,\n",
    "    if both sequences are present in sequence_embeddings_test_dict, then create a dictionary \n",
    "    with key 'true' mapping to a list of the two embeddings [logits_target, logits_ligand].\n",
    "    \n",
    "    Parameters:\n",
    "        test_negatives (list): List of pairs (e.g., (seq_target, seq_ligand)) for negative instances.\n",
    "        test_positives (list): List of pairs (e.g., (seq_target, seq_ligand)) for positive instances.\n",
    "        sequence_embeddings_test_dict (dict): Dictionary mapping sequences (strings) to their embeddings (torch.Tensor).\n",
    "    \n",
    "    Returns:\n",
    "        list: A list where each element is a dictionary of the form:\n",
    "              {'true': [logits_target, logits_ligand]}\n",
    "              Only pairs for which both sequences are found in sequence_embeddings_test_dict are included.\n",
    "    \"\"\"\n",
    "    output_full = []\n",
    "    \n",
    "    # Process negative pairs\n",
    "    for pair in test_negatives:\n",
    "        # Unpack the pair; assuming each pair is a tuple or list of two sequences.\n",
    "        seq_target, seq_ligand = pair\n",
    "        \n",
    "        # Check if both sequences exist in the embeddings dictionary\n",
    "        if seq_target in sequence_embeddings_test_dict and seq_ligand in sequence_embeddings_test_dict:\n",
    "            output_in_file = {}\n",
    "            logits_target = sequence_embeddings_test_dict[seq_target]\n",
    "            logits_ligand = sequence_embeddings_test_dict[seq_ligand]\n",
    "            output_in_file['negative'] = [logits_target, logits_ligand]\n",
    "            output_full.append(output_in_file)\n",
    "    \n",
    "    # Process positive pairs\n",
    "    for pair in test_positives:\n",
    "        seq_target, seq_ligand = pair\n",
    "        \n",
    "        # Check if both sequences exist in the embeddings dictionary\n",
    "        if seq_target in sequence_embeddings_test_dict and seq_ligand in sequence_embeddings_test_dict:\n",
    "            output_in_file = {}\n",
    "            logits_target = sequence_embeddings_test_dict[seq_target]\n",
    "            logits_ligand = sequence_embeddings_test_dict[seq_ligand]\n",
    "            output_in_file['true'] = [logits_target, logits_ligand]\n",
    "            output_full.append(output_in_file)\n",
    "\n",
    "    \n",
    "    return output_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a39b941-0166-42f1-bf99-13326a4f8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_unique_test = 'db_unfiltered/elements_uniqueclassification_aug_types_test.pkl'\n",
    "test_unique_sequences = load_pkl(file_path_unique_test)\n",
    "\n",
    "file_path_test_negatives = 'db_unfiltered/non_interacting_pairsclassification_aug_types_test.pkl'\n",
    "test_negatives = load_pkl(file_path_test_negatives)\n",
    "file_path_test_positives = 'db_unfiltered/interacting_pairsclassification_aug_types_test.pkl'\n",
    "test_positives = load_pkl(file_path_test_positives)\n",
    "\n",
    "embeddings_paths = 'test_types/representations'\n",
    "\n",
    "sequence_embeddings_test_dict = get_sequence_embeddings(test_unique_sequences, embeddings_paths)\n",
    "\n",
    "output_full = create_output_full(test_negatives, test_positives, sequence_embeddings_test_dict)\n",
    "out_path = 'classification_test_RNAFM.p'\n",
    "with open(out_path, 'wb') as f:\n",
    "    pickle.dump(output_full, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce7c1787-8278-48c1-a338-539d8055c865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193308"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49cd86ee-2399-4b92-aa88-ed6060da0374",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_unique_train = 'db_unfiltered/elements_uniqueclassification_aug_types_training.pkl'\n",
    "train_unique_sequences = load_pkl(file_path_unique_train) # list of sequences\n",
    "\n",
    "file_path_train_negatives = 'db_unfiltered/non_interacting_pairsclassification_aug_types_training.pkl'\n",
    "train_negatives = load_pkl(file_path_train_negatives)\n",
    "file_path_train_positives = 'db_unfiltered/interacting_pairsclassification_aug_types_training.pkl'\n",
    "train_positives = load_pkl(file_path_train_positives)\n",
    "embeddings_paths = 'train_types/representations'\n",
    "\n",
    "sequence_embeddings_train_dict = get_sequence_embeddings(train_unique_sequences, embeddings_paths)\n",
    "\n",
    "output_full = create_output_full(train_negatives, train_positives, sequence_embeddings_train_dict)\n",
    "out_path = 'classification_train_RNAFM.p'\n",
    "with open(out_path, 'wb') as f:\n",
    "    pickle.dump(output_full, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7222d259-d13b-41cb-8dff-155c5e9875dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1622240"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e94a4-a6c6-4965-82ec-3e1c90394402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ef338-c325-48ca-9922-b841f7b63856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348dd5e-a27c-4d18-b088-e03ebac68cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127bd764-fad2-48d9-8ad2-11b8723f84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirna-lncrna\n",
    "# train\n",
    "\n",
    "file_path_unique_train = 'db_unfiltered/elements_uniqueclassification_mirna-lncrna_aug_types_training.pkl'\n",
    "train_unique_sequences = load_pkl(file_path_unique_train) # list of sequences\n",
    "\n",
    "file_path_train_negatives = 'db_unfiltered/non_interacting_pairsclassification_mirna-lncrna_aug_types_training.pkl'\n",
    "train_negatives = load_pkl(file_path_train_negatives)\n",
    "\n",
    "file_path_train_positives = 'db_unfiltered/interacting_pairsclassification_mirna-lncrna_aug_types_training.pkl'\n",
    "train_positives = load_pkl(file_path_train_positives)\n",
    "\n",
    "embeddings_paths = 'mirna-lncrna_train/representations'\n",
    "\n",
    "sequence_embeddings_train_dict = get_sequence_embeddings(train_unique_sequences, embeddings_paths)\n",
    "\n",
    "output_full = create_output_full(train_negatives, train_positives, sequence_embeddings_train_dict)\n",
    "out_path = 'classification_mirna_lncrna_train_RNAFM.p'\n",
    "with open(out_path, 'wb') as f:\n",
    "    pickle.dump(output_full, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72543562-1950-49f1-850c-b931f1d532fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirna-lncrna\n",
    "# test\n",
    "\n",
    "file_path_unique_test = 'db_unfiltered/elements_uniqueclassification_mirna-lncrna_aug_types_test.pkl'\n",
    "test_unique_sequences = load_pkl(file_path_unique_test)\n",
    "\n",
    "file_path_test_negatives = 'db_unfiltered/non_interacting_pairsclassification_mirna-lncrna_aug_types_test.pkl'\n",
    "test_negatives = load_pkl(file_path_test_negatives)\n",
    "file_path_test_positives = 'db_unfiltered/interacting_pairsclassification_mirna-lncrna_aug_types_test.pkl'\n",
    "test_positives = load_pkl(file_path_test_positives)\n",
    "\n",
    "embeddings_paths = 'mirna-lncrna_test/representations'\n",
    "\n",
    "sequence_embeddings_test_dict = get_sequence_embeddings(test_unique_sequences, embeddings_paths)\n",
    "\n",
    "output_full = create_output_full(test_negatives, test_positives, sequence_embeddings_test_dict)\n",
    "out_path = 'classification_mirna_lncrna_test_RNAFM.p'\n",
    "with open(out_path, 'wb') as f:\n",
    "    pickle.dump(output_full, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff76cd4-86be-4dd3-a1db-356d392fcbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7747d40-04d4-45eb-80f3-1a8ca5c2c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirna-mirna\n",
    "# train\n",
    "file_path_unique_train = 'db_unfiltered/elements_uniqueclassification_mirna-mirna_aug_types_training.pkl'\n",
    "train_unique_sequences = load_pkl(file_path_unique_train) # list of sequences\n",
    "\n",
    "file_path_train_negatives = 'db_unfiltered/non_interacting_pairsclassification_mirna-mirna_aug_types_training.pkl'\n",
    "train_negatives = load_pkl(file_path_train_negatives)\n",
    "\n",
    "file_path_train_positives = 'db_unfiltered/interacting_pairsclassification_mirna-mirna_aug_types_training.pkl'\n",
    "train_positives = load_pkl(file_path_train_positives)\n",
    "\n",
    "embeddings_paths = 'mirna-mirna_train/representations'\n",
    "\n",
    "sequence_embeddings_train_dict = get_sequence_embeddings(train_unique_sequences, embeddings_paths)\n",
    "\n",
    "output_full = create_output_full(train_negatives, train_positives, sequence_embeddings_train_dict)\n",
    "out_path = 'classification_mirna_mirna_train_RNAFM.p'\n",
    "with open(out_path, 'wb') as f:\n",
    "    pickle.dump(output_full, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5494310c-0889-47f3-b641-1fb5c3fd3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirna-mirna\n",
    "# test\n",
    "file_path_unique_test = 'db_unfiltered/elements_uniqueclassification_mirna-mirna_aug_types_test.pkl'\n",
    "test_unique_sequences = load_pkl(file_path_unique_test)\n",
    "\n",
    "file_path_test_negatives = 'db_unfiltered/non_interacting_pairsclassification_mirna-mirna_aug_types_test.pkl'\n",
    "test_negatives = load_pkl(file_path_test_negatives)\n",
    "file_path_test_positives = 'db_unfiltered/interacting_pairsclassification_mirna-mirna_aug_types_test.pkl'\n",
    "test_positives = load_pkl(file_path_test_positives)\n",
    "\n",
    "embeddings_paths = 'mirna-mirna_test/representations'\n",
    "\n",
    "sequence_embeddings_test_dict = get_sequence_embeddings(test_unique_sequences, embeddings_paths)\n",
    "\n",
    "output_full = create_output_full(test_negatives, test_positives, sequence_embeddings_test_dict)\n",
    "out_path = 'classification_mirna_mirna_test_RNAFM.p'\n",
    "with open(out_path, 'wb') as f:\n",
    "    pickle.dump(output_full, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ca32e7-9d61-4f36-a069-dd98ac876cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949123dd-2159-400d-9aa0-a4aa3a8e0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirna-snorna\n",
    "# train\n",
    "file_path_unique_train = 'db_unfiltered/elements_uniqueclassification_mirna-snorna_aug_types_training.pkl'\n",
    "train_unique_sequences = load_pkl(file_path_unique_train) # list of sequences\n",
    "\n",
    "file_path_train_negatives = 'db_unfiltered/non_interacting_pairsclassification_mirna-snorna_aug_types_training.pkl'\n",
    "train_negatives = load_pkl(file_path_train_negatives)\n",
    "\n",
    "file_path_train_positives = 'db_unfiltered/interacting_pairsclassification_mirna-snorna_aug_types_training.pkl'\n",
    "train_positives = load_pkl(file_path_train_positives)\n",
    "\n",
    "embeddings_paths = 'mirna-snorna_train/representations'\n",
    "\n",
    "sequence_embeddings_train_dict = get_sequence_embeddings(train_unique_sequences, embeddings_paths)\n",
    "\n",
    "output_full = create_output_full(train_negatives, train_positives, sequence_embeddings_train_dict)\n",
    "out_path = 'classification_mirna_snorna_train_RNAFM.p'\n",
    "with open(out_path, 'wb') as f:\n",
    "    pickle.dump(output_full, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04768dc5-7134-431f-a58c-e67998d32050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirna-snorna\n",
    "# test\n",
    "file_path_unique_test = 'db_unfiltered/elements_uniqueclassification_mirna-snorna_aug_types_test.pkl'\n",
    "test_unique_sequences = load_pkl(file_path_unique_test)\n",
    "\n",
    "file_path_test_negatives = 'db_unfiltered/non_interacting_pairsclassification_mirna-snorna_aug_types_test.pkl'\n",
    "test_negatives = load_pkl(file_path_test_negatives)\n",
    "file_path_test_positives = 'db_unfiltered/interacting_pairsclassification_mirna-snorna_aug_types_test.pkl'\n",
    "test_positives = load_pkl(file_path_test_positives)\n",
    "\n",
    "embeddings_paths = 'mirna-snorna_test/representations'\n",
    "\n",
    "sequence_embeddings_test_dict = get_sequence_embeddings(test_unique_sequences, embeddings_paths)\n",
    "\n",
    "output_full = create_output_full(test_negatives, test_positives, sequence_embeddings_test_dict)\n",
    "out_path = 'classification_mirna_snorna_test_RNAFM.p'\n",
    "with open(out_path, 'wb') as f:\n",
    "    pickle.dump(output_full, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a25226-b0fa-435d-8688-a3f995bb4252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24852dbe-4205-4f22-98b3-6b34b1512a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
